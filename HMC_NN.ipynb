{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS MATERIAL IS UNDER DEVELOPMENT AND IS NOT YET IN A READABLE STATE. IF YOU COPY OR DISTRIBUTE THIS USE THIS MATERIAL, I WOULD APPRECIATE IF YOU ADD A CITATION. \n",
    "\n",
    "BR, \n",
    "Jonne P.\n",
    "\n",
    "\n",
    "The purpose of this tutorial is to given an introductory on the subject of Hamiltonian Monte Carlo and deep neural networks. We will begin this tutorial by going through the basics concepts from mechanics and propability theory, which are required for understanding the subject.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Neural networks\n",
    "\n",
    "### 1.1 Perceptron\n",
    "### 1.2 From single perceptron to multiple neurons\n",
    "\n",
    "## 2. Convolutional neural networks / deep learning (CNN)\n",
    "\n",
    "### 2.1 Texture distribution as a feature\n",
    "### 2.2 Convolution \n",
    "### 2.3 Deep learning networks / hierarchical learning\n",
    "\n",
    "## 3. Training neural networks: back propagation\n",
    "\n",
    "### 3.1 gradient descent\n",
    "### 3.2 stochastic gradient descent\n",
    "### 3.3 back propagation algorithm: derivation \n",
    "\n",
    "## 4. MECHANICS\n",
    "\n",
    "### 4.1 Newtonian / classical mechanics\n",
    "\n",
    "In classical or Newtonian mechanics, the elementary relation between an object and an exerted force is descriobed by the Newton's 2nd law: \n",
    "\n",
    "$$F=ma,$$\n",
    "\n",
    "which relates the force $F$ exerted on an object with mass $m$ and acceleration $a$. Force is measured in the units of Newton's (N) and 1 Newton can be interpreted in the following way: \"One Newton, is the force required for causing an acceleration of $1 \\frac{m}{s^2}$ on an object with mass of 1 kg\". To give an example, say we have an object with mass 10 kg and we wish to cause acceleration of $5\\frac{m}{s^2}$ on the object. We thus need to apply the force:\n",
    "\n",
    "$$F = 10\\cdot 5 \\;\\frac{kg\\cdot m}{s^2} = 50 N,$$\n",
    "\n",
    "on the object. So far, we have been talking about a single object or a particle, but what about when we have a system of particles or objects? How can we model the dynamics of a system of particles? Well, we simply sum up all the forces exerted on the system (by system I mean all the particles) and so the dynamics of group of particles is defined by the equation: \n",
    "\n",
    "$$\\sum_{i=1}^n F_i = \\sum_{i=1}^n m_i a_i.$$\n",
    "\n",
    "This equation includes both the internal and external forces on the system and constraint forces. Our task now is to use this equation and explain the dynamics of the system. One can quickly agree however, that this is a rather difficult problem: we have a multitude of particles in our hand with many different forces acting on the system. It is not thus easy to explain the dynamics of such a system using Newtonian mechanics. This is where we bring in the Lagrangian and Hamiltonian mechanics, which are equivalent to the Newtonian mechanics in describing the dynamics of the system, but they offer a much easier way to do this. Plus, they give us more information on the dynamics of the system than Newtonian mechanics does. \n",
    "\n",
    "#### Example of Newtonian mechanics: Atwood machine\n",
    "\n",
    "To give an example of Newtonian mechanics, we will consider the classical example of the Atwood machine (1784, by George Atwood). The Atwoow machine is a laboratory experiment for verifying the mechanical laws of motion in a constant acceleration case.\n",
    "\n",
    "In a basic Atwood machine example, we consider two objects with masses $m_1$ kg and $m_2$ kg, connected by an inextensible massless string over an ideal massless pulley. In this example, we wish to investigate the acceleration $a$ of the 'object-string-object'-system.\n",
    "\n",
    "First of all, lets find the forces acting on the system. We assume for the sake of simplicity, that the string produces a consant force of $T$ to the two objects. Also, we assume that the acceleration of the system is positive ($a > 0$) when object with mass $m_1$ is falling down and negative $(a < 0)$ if it is rising up. In addition to the string force $T$, the forces acting on the objects are their weights due to gravity, that is: \n",
    "\n",
    "$$W_1 = m_1 g\\;\\;\\;\\;\\;\\;\\;\\;\\; W_2 = m_2 g,$$\n",
    "\n",
    "where $g = 9.8 \\;\\frac{m}{s^2}$ is the acceleration due to gravity. So the total forces acting on the two objects are: \n",
    "\n",
    "$$W_1 - T= m_1 g - T = m_1 a\\;\\;\\;\\;\\;\\;\\;\\;\\; T - W_2= T-m_2 g = m_2 a,$$\n",
    "\n",
    "and if we add these two equations we get: \n",
    "\n",
    "$$m_1 g -T + T-m_2 g = m_1 a + m_2 a,$$\n",
    "\n",
    "from which we easily get that the acceleration of the 'object-string-object'-system is: \n",
    "\n",
    "$$a = g\\frac{m_1-m_2}{m_1+m_2}.$$\n",
    "\n",
    "To make this more concrete with real numbers, let $m_1 = 1.1$ kg and $m_2 = 1$ kg. We thus get that the acceleration of the system is: \n",
    "\n",
    "$$a = 9.8 \\;\\frac{m}{s^2}\\frac{(1.1 - 1) kg}{(1.1 + 1) kg} \\approx 0.048 \\times 9.8 \\frac{m}{s^2} \\approx 0.47 \\frac{m}{s^2}.$$\n",
    "\n",
    "To get a better feeling of Atwood machine, please refer to the simulation by Andrew Duffy: [Atwood machine](http://physics.bu.edu/~duffy/HTML5/Atwoods_machine.html). \n",
    "\n",
    "### 4.2 Lagrangian mechanics\n",
    "\n",
    "In mechanics, we are interested in the motion of objects: how fast a car drives, how the earth orbits the sun, the oscillating motion of a pendulum, etc. From a simple applied point of view, Lagrangian mechanics is just a different way to approach a given mechanical problem. Let us take as an example the motion of a pendulum. Our goal is to describe how the pendulum will move.\n",
    "\n",
    "In Newtonian mechanics (the “normal” mechanics taught in high school), we would start by drawing a diagram with multiple arrows for all the forces which are acting on the pendulum. We can then find how the pendulum is moving by using Newton’s second law: F=ma. More generally in Newtonian mechanics, we take Newton’s three laws as fundamental laws of nature and try to derive everything else from there. It is centered around forces, since these are ultimately used to figure out the trajectories.\n",
    "\n",
    "In Lagrangian mechanics, things work differently. To obtain the same result, we start by calculating the kinetic and potential energy of the pendulum. Instead of Newton’s three laws, we assume that there is another fundamental law of nature: the principle of least action. According to this principle, we can calculate the motion by minimizing a certain quantity, called the action, that is related to the two forms of energy mentioned above. Lagrangian mechanics therefore is centered around energies. Forces are no longer needed to determine the motion of objects.\n",
    "\n",
    "Both variants of course lead to the same trajectory for the pendulum. It can be shown more generally that these two formalism are equivalent. However, they each are better suited for certain types of problems.\n",
    "\n",
    "Besides being more convenient to solve some problems, there is a much deeper reason for why it is a good idea to introduce Lagrangian mechanics. It turns out that many of the fundamental laws of physics can be described by such a principle of least action. To give you a taste, here is the so called “Lagrangian” of the standard model (the action we try to minimize in the principle of least action is the integral of this beast):\n",
    "\n",
    "As in Newtonian mechanics, we begin the Lagrangian formulation from the equation (with vector quantities): \n",
    "\n",
    "$$\\sum_{i=1}^n F_i = \\sum_{i=1}^n m_i a_i,$$\n",
    "\n",
    "which describes the dynamics of a system of $n$ particles. Furthermore, we decompose the forces into two components, the applied forces $F_i^{(a)}$ and forces due to constraints $f_i$, and so we get: \n",
    "\n",
    "$$\\sum_{i=1}^n \\left(F_i^{(a)} + f_i\\right) = \\sum_{i=1}^n m_i a_i = \\sum_{i=1}^n \\dot{p}_i ,$$\n",
    "\n",
    "where we have included the moment $p_i=m_iv_i$ notation, where $v_i$ is the velocity of particle $i$. That is, \n",
    "\n",
    "$$\\dot{p}_i = \\frac{d p_i}{d t}=\\frac{d (m_iv_i)}{d t}=m_i\\frac{d v_i}{d t}=m_ia_i.$$\n",
    "\n",
    "Thus we have for the equation of motion of the system that: \n",
    "\n",
    "$$\\sum_{i=1}^n \\left(F_i^{(a)} + f_i - \\dot{p}_i \\right) = \\textbf{0}.$$\n",
    "\n",
    "An example of $F_i^{(a)}$ could be caused e.g.via a push by an external agent to the system. An example of a force of constraint $f_i$ could be e.g. gravity which forces particle $i$ to stay on a plane. Another easier example is perhaps to think of a rigid-body movement. The whole system experiences a force by an external agent and thus each particle $i$ experiences a force, and each particle of the system is constrained to its own relative position wit respect to other particles of the system. \n",
    "\n",
    "Next, we introduce more factors into the game. The symbols $r_1, r_2, ..., r_n$ refer to the position vectros of the $n$ particles and time is denoted as $t$. The constraints imposed on the system on $n$ particles are represented by the set of equations: \n",
    "\n",
    "$$f(r_1, r_2, r_3, ..., t)=0,$$\n",
    "\n",
    "which are called holonomic constraints. If we have $k$ of these holonomic constraint equations, then we can use them to eliminate $k$ of the $3n$ (each particle has three coordinates) position coordinates of the particle system and we are left with $3n-k$ coordinates which we can select **independently** with respect to each other. These remaining $3n-k$ coordinates (denoted as $q_i$) are called generalized coordinates and they implicitly contain the constraints of the system, which is defined by the $k$ holonimic equations. That is, we now have: \n",
    "\n",
    "$$\n",
    "\\begin{matrix} \n",
    "r_1 = r_1(q_1, q_2, ..., q_{3n-k}, t) \\\\\n",
    "\\vdots \\\\\n",
    "r_n = r_n(q_1, q_2, ..., q_{3n-k}, t).\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Next we introduce the concept of virtual displacement, which refers to the a change in the configuration of the system as the result of any arbitrary infinitesimal change of the coordinates $\\delta r_i$, consistent with the forces and constraints imposed on the system at the given time instant $t$. By now taking the dot product of above equation with $\\delta r_i$ in each particle, we get that:\n",
    "\n",
    "$$\\sum_{i=1}^n \\left(F_i^{(a)} + f_i - \\dot{p}_i \\right)\\cdot \\delta r_i = 0.$$\n",
    "\n",
    "By restricting ourselves to a system for which the virtual work of the forces of constraint vanishes we get: \n",
    "\n",
    "$$\\sum_{i=1}^n \\left(F_i^{(a)}- \\dot{p}_i \\right)\\cdot \\delta r_i = 0,$$\n",
    "\n",
    "which is often called D'Alembert's principle. By now making a series of algebraic manipulations and subsitutions to this equation (more on these e.g. in Goldstein) the D'Alembert's principle becomes: \n",
    "\n",
    "$$\\sum_{j} \\left\\{ \\frac{d}{dt} \\left[ \\frac{\\partial}{\\partial\\dot{q}_j}\\left(\\sum_i \\frac12 m_iv_i^2\\right) \\right] - \\frac{\\partial}{\\partial q_j}\\left(\\sum_i \\frac12 m_iv_i^2\\right) -Q_j \\right\\} \\delta q_j=0,$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\dot{q}_j = \\frac{d q_j}{dt}\\;\\;\\;\\text{and}\\;\\;\\;Q_j = \\sum_i F_i \\cdot \\frac{\\partial r_i}{\\partial q_j}.$$\n",
    "\n",
    "The symbol $Q_j$ is known as the generalized force in mechanics. We now recognize the two sums involving the particle masses are the total kinetic energy $T=\\sum_i \\frac12 m_i v_i^2$ of the system and so we can write the D'Alembert's principle as: \n",
    "\n",
    "$$\\sum_{j} \\left\\{ \\frac{d}{dt} \\left[ \\frac{\\partial T}{\\partial\\dot{q}_j} \\right] - \\frac{\\partial T}{\\partial q_j} -Q_j \\right\\} \\delta q_j=0.$$\n",
    "\n",
    "If one has experience with calculus of variation the above equations should start to look familiar. Anyway, we now remember that since the generalized coordinates $q_j$ could be arbitrarily chosen due to independency, it follows that in order for the above equation to hold it must be that each of the terms in the sum vanish, that is: \n",
    "\n",
    "$$\\frac{d}{dt} \\left[ \\frac{\\partial T}{\\partial\\dot{q}_j} \\right] - \\frac{\\partial T}{\\partial q_j} -Q_j=0\\;\\;\\;\\forall j .$$\n",
    "\n",
    "Furthermore, if all the applied forces are derivable from a scalar potential energy function $V$, that is: \n",
    "\n",
    "$$F_i^{(a)}=-\\nabla_i V,$$\n",
    "\n",
    "then we have that: \n",
    "\n",
    "$$Q_j = \\sum_i F_i \\cdot \\frac{\\partial r_i}{\\partial q_j} = -\\sum_i \\nabla_i V\\cdot \\frac{\\partial r_i}{\\partial q_j} = -\\frac{\\partial V}{\\partial _j},$$\n",
    "\n",
    "and also assuming that $V$ does not depend on time $t$ and the generalized velocities $\\dot{q}_j$ (an assumption perfectly valid in many real applications) we get that the individual terms in the sum get the form: \n",
    "\n",
    "$$\\frac{d}{dt} \\left[ \\frac{\\partial (T-V)}{\\partial\\dot{q}_j} \\right] - \\frac{\\partial (T-V)}{\\partial q_j} =0\\;\\;\\;\\forall j ,$$\n",
    "\n",
    "or that \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial\\dot{q}_j} \\right) - \\frac{\\partial L}{\\partial q_j} =0\\;\\;\\;\\forall j,\n",
    "}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which is referred to as the **Lagrange's equations** of motion with the Lagrangian function $L=T-U$, that the difference of kinetic and potential energy of the system.  \n",
    "\n",
    "So what have achieved now? We have transformed the equations describing the mechanics of the system into a different yet equivalent format expressed explicitly in terms of energies of the system. In the Newtonian equations, we were dealing with complicated equations of forces et cetera. Now, we are expressing the same dynamics of the system with energies. We thus only need to know the Lagrangian of the physical system we are interested about and we can then use the Lagrange's equations of motion to find out its dynamics. In many cases in physics, this makes the imvestigation of the system's mechanics a lot easier. There is also another way of producing Lagrange's equations using the variational approach, but we will not cover this in this tutorial and it can be found in many books on mechanics (e.g. Goldstein).\n",
    "\n",
    "#### Example of Lagrangian mechanics: Atwood machine\n",
    "\n",
    "We will now go through the same Atwood machine example as we did with Newtonian mechanics. Recall that in the Lagrangian mechanics, we approach the problem by considering system energies via the Lagrangian function. So, lets find it out, we need to find the kinetic energy $T$ and the potential energy $V$ of the system. Note also that the Lagrangian formulation relies on conservative forces, a property valid in many physical applications (e.g. gravity, electromagnetism).\n",
    "\n",
    "First of all, we must recognize the coordinates of the system. We denote $x$ to be the distance from the pulley to object with mass $m_1$. The corresponding distance for object 2 is $l-x$, where $l$ is the length of the inextensible string connecting the two objects.  We assume the height from the ground to the pulley is $h$. Since the string is inextensible we see that the only variable coordinate in this system is $x$, that is $x$ determines the position of objects 1 and 2 at all time. \n",
    "\n",
    "So what are holonomic constraint equations? What are the generalized coordinates? We denote the height (position coordinate) of object 1 from ground as $h-x$ and the height of object 2 from the ground as $h-(l-x)$. The constraint we have here for the position vectors $r_1 = x, r_2 = l-x$ is that the length of the string must always be $l$, that is: \n",
    "\n",
    "$$r_1+r_2 = x+l-x = l,$$\n",
    "\n",
    "which is trivially the case and so the only 'generalized' coordinate is $x$. Next, we notice that the kinetic energy of the Atwood machine is given by: \n",
    "\n",
    "$$T = \\frac12 m_1\\dot{x}^2 + \\frac12 m_2\\dot{x}^2,$$\n",
    "\n",
    "where $\\dot{x}=\\frac{dx}{dt}$ is the velocity of the 'object-string-object' system. The potential energy of the system is given by: \n",
    "\n",
    "$$V = m_1 g (h-x) + m_2 g (h-(l-x)),$$\n",
    "\n",
    "and so the Lagrangian function is: \n",
    "\n",
    "$$L(x,\\dot{x}) = T(x)-V(x) = \\frac12 m_1\\dot{x}^2 + \\frac12 m_2\\dot{x}^2 - m_1g(h-x) - m_2g(h-(l-x)),$$\n",
    "\n",
    "or\n",
    "\n",
    "$$L(x,\\dot{x}) = \\frac12 (m_1+m_2)\\dot{x}^2 + g(m_1-m_2)x + C,$$\n",
    "\n",
    "where $C$ is a constant value. We now recognize the components needed for the Lagrangian equations as: \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\dot{x}}=(m_1+m_2)\\dot{x}\\;\\;\\;\\text{and}\\;\\;\\;\\frac{\\partial L}{\\partial x}=g(m_1-m_2),$$\n",
    "\n",
    "and so we get from Lagrangian equation: \n",
    "\n",
    "$$\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial\\dot{x}} \\right) - \\frac{\\partial L}{\\partial x} = \\frac{d}{dt} \\left( (m_1+m_2)\\dot{x} \\right) - g(m_1-m_2) = (m_1+m_2)\\ddot{x}- g(m_1-m_2)=0,$$\n",
    "\n",
    "that is: \n",
    "\n",
    "$$\\ddot{x}=a=g\\frac{m_1-m_2}{m_1+m_2},$$\n",
    "\n",
    "which is as expected the exactly same result we got with Newtonian mechanics. This particular example does not highlight the benefits of Lagrangian mechanics but it is much easier to use Lagrangian than Newtonian mechanics in more complicated applications. The point of this example is to simply illustrate how we can derive the same equations of motion for Atwood machine knowing only the Lagrangian of the system energies. \n",
    "\n",
    "\n",
    "### 4.3. HAMILTONIAN MECHANICS\n",
    "\n",
    "We still have one more (and the key) concept to deal with in addition to Newtonian and Lagrangian mechanis, which is the Hamiltonian way of dealing with mechanics. Hamiltonian mechanics also lays foundations for the formulations of statistical and quantum mechanics. Like before, the Hamiltonian formulation of mechanics is equivalent with the two previous one but now instead of expressing the energy function as a function of generalized coordinates and velocities and time $(q,\\dot{q},t)$, we wish to express the system energy in terms of generalized coordinates, momentum and time $(q, p, t)$. That said, we now define the generalized or conjugate momentum as: \n",
    "\n",
    "$$p_i = \\frac{\\partial L(q,\\dot{q},t)}{\\partial \\dot{q}_i}.$$\n",
    "\n",
    "Furthermore, substituting this definition into the Lagrange's equations we get: \n",
    "\n",
    "$$\\frac{dp_i}{dt} - \\frac{\\partial L}{\\partial q_j} =0\\;\\;\\rightarrow\\;\\;\\dot{p}_i = \\frac{\\partial L}{\\partial q_j}.$$\n",
    "\n",
    "In order to produce the new energy function (called the Hamiltonian), we apply the Legendre transformation for the Lagrangian, which is tailored for just this type of change of variables. Consider a function of only two variables $f(x,y)$, so that a differential of $f$ has the form: \n",
    "\n",
    "$$df = u\\,dx + v\\,dy,$$\n",
    "\n",
    "where \n",
    "\n",
    "$$u = \\frac{\\partial f}{\\partial x},\\;\\;\\;v = \\frac{\\partial f}{\\partial y}.$$\n",
    "\n",
    "We want now to change the basis of description from $x,y$ t a new set of variables $u,y$, so that differential quantities are expressed in terms of the differentials $du$ and $dy$. Let $g$ be a fucntion of $u$ and $y$ defined as: \n",
    "\n",
    "$$g(u,y) = f(x,y)-ux.$$\n",
    "\n",
    "A differential of $g$ is then given as: \n",
    "\n",
    "$$dg = df - u\\,dx - x\\,du = u\\,dx + v\\,dy - u\\,dx - x\\,du = v\\,dy - x\\,du$$\n",
    "\n",
    "which is now exactly in the form desired. The quantities $x$ and $v$ are now functions of the variables $u$ and $y$ given by: \n",
    "\n",
    "$$x = -\\frac{\\partial g(u,y)}{\\partial u},\\;\\;\\;v = \\frac{\\partial g(u,y)}{\\partial y}.$$\n",
    "\n",
    "Next, we define the negative of the Hamiltonian function as:\n",
    "\n",
    "$$-H(q,p,t) = L(q,\\dot{q}, t)-\\sum_j \\dot{q}_jp_j$$\n",
    "\n",
    "and we apply the Legendre transformation to multivariable case. First, we have the differential of the Lagrangian:  \n",
    "\n",
    "$$dL = dL(q_1, ..., q_n, \\dot{q}_1, ..., \\dot{q}_n, t) = \\sum_{j}\\dot{p}_j\\,dq_j + \\sum_jp_j\\,d\\dot{q}_j+\\frac{\\partial L}{\\partial t}\\,dt,$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\dot{p}_j = \\frac{\\partial L}{\\partial q_j},\\;\\;\\;p_j = \\frac{\\partial L}{\\partial \\dot{q}_j}.$$\n",
    "\n",
    "We now use the Legendre transformation to get the negative of the Hamiltonian as (I use negative notation to be consistent with the Legendre transformation above): \n",
    "\n",
    "$$-H(q_1, ..., q_n, p_1, ..., p_n, t) = L(q_1, ..., q_n, \\dot{q}_1, ..., \\dot{q}_n, t)-\\sum_j p_j\\dot{q}_j,$$\n",
    "\n",
    "and the differential of this is: \n",
    "\n",
    "$$-dH= dL-\\sum_j p_j\\,d\\dot{q}_j-\\sum_j \\dot{q}_j\\,dp_j,$$\n",
    "\n",
    "or\n",
    "\n",
    "$$-dH= \\sum_{j}\\dot{p}_j\\,dq_j + \\sum_jp_j\\,d\\dot{q}_j-\\sum_j p_j\\,d\\dot{q}_j-\\sum_j \\dot{q}_j\\,dp_j+\\frac{\\partial L}{\\partial t}\\,dt=\\sum_{j}\\dot{p}_j\\,dq_j -\\sum_j \\dot{q}_j\\,dp_j+\\frac{\\partial L}{\\partial t}\\,dt.$$\n",
    "\n",
    "Also, since we know that $-H = -H(q_1, ..., q_n, p_1, ..., p_n, t)$ we get the differential as: \n",
    "\n",
    "$$-dH = \\sum_j \\frac{\\partial (-H)}{\\partial q_j}\\,dq_j + \\sum_j \\frac{\\partial (-H)}{\\partial p_j}\\,dp_j+\\frac{\\partial L}{\\partial t}\\,dt.$$\n",
    "\n",
    "Comparing the two different differential expressions for $-H$, we see that: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\dot{p}_j = \\frac{\\partial (-H)}{\\partial q_j}\\Rightarrow -\\dot{p}_j = \\frac{\\partial H}{\\partial q_j}\\\\\n",
    "-\\dot{q}_j = \\frac{\\partial (-H)}{\\partial p_j}\\Rightarrow \\dot{q}_j = \\frac{\\partial H}{\\partial p_j}.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Also, from the partial derivative of the Lagrangian w.r.t time: \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial t} = \\sum_j \\frac{\\partial L}{\\partial q_j}\\frac{\\partial q_j}{\\partial t}+\\sum_j \\frac{\\partial L}{\\partial \\dot{q}_j}\\frac{\\partial \\dot{q}_j}{\\partial t} + \\frac{\\partial L}{\\partial t}=\\sum_j \\dot{p}_j\\,\\dot{q}_j+\\sum_j p_j\\,\\ddot{q}_j + \\frac{\\partial L}{\\partial t}$$\n",
    "\n",
    "we see that\n",
    "\n",
    "$$\\frac{\\partial(-H)}{\\partial t}=-\\frac{\\partial H}{\\partial t}= \\frac{\\partial L}{\\partial t} - \\sum_j \\frac{\\partial p_j}{\\partial t}\\,\\dot{q}_j - \\sum_j p_j\\,\\frac{\\partial \\dot{q}_j}{\\partial t} = \\sum_j \\dot{p}_j\\,\\dot{q}_j+\\sum_j p_j\\,\\ddot{q}_j + \\frac{\\partial L}{\\partial t} - \\sum_j \\dot{p}_j\\,\\dot{q}_j - \\sum_j p_j\\,\\ddot{q}_j=\\frac{\\partial L}{\\partial t}$$\n",
    "\n",
    "Thus we have derived the (canonical) **Hamiltonian equations** of motion: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "-\\dot{p}_j = \\frac{\\partial H}{\\partial q_j}\\\\\n",
    "\\dot{q}_j = \\frac{\\partial H}{\\partial p_j}\\;\\;\\;\\;\\;\\;\\;\\;\\forall j\\\\\n",
    "-\\frac{\\partial L}{\\partial t}=\\frac{\\partial H}{\\partial t}\n",
    "}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Few last words about the Hamiltonian energy function. Recall that in the Lagrangian mechanics we defined the Lagrangina function as the difference between system kinetic and potential energy $L=T-V$. What is the case with the Hamiltonian $H$? Lets take a closer look at this. For a very wide range of systems and sets of generalized coordinates, the Lagrangian can be decomposed as regards its functional behavior in the $\\dot{q}$ variables as: \n",
    "\n",
    "$$L(q,\\dot{q},t)=L_0(q,t)+L_1(q,\\dot{q},t)+L_2(q,\\dot{q},t),$$\n",
    "\n",
    "where $L_2$ is a homogeneous function of the second degree (not merely quadratic) in $\\dot{q}$, while $L_1$ is homogeneous of the first degree in $\\dot{q}$. Recall that the Euler's theorem states that if a function $f(x_1, x_2, ...)$ is a homogeneous function of degree $m$ in the variables $x_j$, then:\n",
    "\n",
    "$$\\sum_j x_j \\frac{\\partial f}{\\partial x_j}=nf.$$\n",
    "\n",
    "There is no reason intrinsic to mechanics that requires the Lagrangian to conform to the above decomposition, but in fact it does for most problems of interest. The Lagrangian has this form when the forces are derivable from a potential not involving the velocities. \n",
    "\n",
    "With these in mind, we apply the Euler's theorem and the Lagrangian decomposition to the Hamiltonian to get: \n",
    "\n",
    "$$H=\\sum_j \\dot{q}_jp_j-L=\\sum_j \\dot{q}_j\\frac{\\partial L}{\\partial \\dot{q}_j}-L=\\sum_j \\dot{q}_j\\frac{\\partial L_0}{\\partial \\dot{q}_j}+\\sum_j \\dot{q}_j\\frac{\\partial L_1}{\\partial \\dot{q}_j}+\\sum_j \\dot{q}_j\\frac{\\partial L_2}{\\partial \\dot{q}_j}-L_0-L_1-L_2=L_1+2L_2-L_0-L_1-L_2=L_2-L_0.$$\n",
    "\n",
    "If further the transformation equations defining the generalized coordinates are independent of time $t$ and the potential energy of the system is independent of the generalized velocities $\\dot{q}$, then we have that $L_2=T$ and $L_0=-V$ and so the Hamiltonian is: \n",
    "\n",
    "$$H=L_2-L_0=T-(-V)=T+V=E,$$\n",
    "\n",
    "where $E$ is the total mechanical energy of the system. To summarize the mechanics we have been going through, the following table illustrates the main concepts between the three approaches to mechanics: \n",
    "\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\textbf{Mechanics} & \\text{Newtonian} & \\text{Lagrangian} & \\text{Hamiltonian} \\\\\n",
    "\\hline \n",
    "\\begin{matrix} \n",
    "\\textbf{Fundamental}\\\\\n",
    "\\textbf{concepts}\n",
    "\\end{matrix} & \\begin{matrix} \n",
    "\\text{System forces, force diagrams} \\\\\n",
    "\\text{constraints}\n",
    "\\end{matrix} & \\begin{matrix} \n",
    "\\text{System energies}\n",
    "\\end{matrix} & \\begin{matrix} \n",
    "\\text{System energies}\n",
    "\\end{matrix} \\\\\n",
    "\\hline \n",
    "\\begin{matrix} \n",
    "\\textbf{Variables}\\\\\n",
    "\\textbf{of interest}\n",
    "\\end{matrix} & \\begin{matrix} \n",
    "\\text{position coordinates r,}\\\\\n",
    "\\text{velocities v}\n",
    "\\end{matrix} & \\begin{matrix} \n",
    "\\text{generalized coordinates q,}\\\\\n",
    "\\text{generalized velocities v}\n",
    "\\end{matrix} & \\begin{matrix} \n",
    "\\text{generalized coordinates q,}\\\\\n",
    "\\text{generalized momentum p}\n",
    "\\end{matrix} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "#### Example of Hamiltonian mechanics: Atwood machine\n",
    "\n",
    "Let us now again calculate the equations of motion for the Atwood machine, but this time using Hamiltonian mechanics. We recall from earlier that the Lagrangian for this system was given by: \n",
    "\n",
    "$$L(x,\\dot{x}) = \\frac12 (m_1+m_2)\\dot{x}^2 + g(m_1-m_2)x + C.$$\n",
    "\n",
    "The generalized momentum in this case is: \n",
    "\n",
    "$$p = \\frac{\\partial L}{\\partial \\dot{x}}=(m_1+m_2)\\dot{x}\\;\\;\\Rightarrow\\;\\;\\dot{x}=\\frac{p}{m_1+m_2},$$\n",
    "\n",
    "and so the Hamiltonian for this conservative (time independent) problem is: \n",
    "\n",
    "$$H(x,p) = \\dot{x}p - L(x,\\dot{x})=\\frac{p^2}{m_1+m_2}-\\frac12 (m_1+m_2)\\left(\\frac{p}{m_1+m_2}\\right)^2 - g(m_1-m_2)x + C=\\frac12 \\frac{p^2}{m_1+m_2} - g(m_1-m_2)x + C,$$\n",
    "\n",
    "that is the Hamiltonian is: \n",
    "\n",
    "$$H(x,p) =\\frac12 \\frac{p^2}{m_1+m_2} - g(m_1-m_2)x + C.$$\n",
    "\n",
    "Applying the Hamiltonian equations of motion we get:\n",
    "\n",
    "$$\\dot{p}=-\\frac{\\partial H}{\\partial x} = g(m_1-m_2)\\;\\;\\;\\text{and}\\;\\;\\;\\dot{x}=\\frac{\\partial H}{\\partial p}=\\frac{p}{m_1+m_2}.$$\n",
    "\n",
    "We take the time derivative of the second equation to get: \n",
    "\n",
    "$$\\frac{\\partial \\dot{x}}{\\partial t}=\\ddot{x}=\\frac{\\dot{p}}{m_1+m_2},$$\n",
    "\n",
    "and plugging in the expression for $\\dot{p}$ we get again that: \n",
    "\n",
    "$$a = \\ddot{x}=\\frac{\\dot{p}}{m_1+m_2}=g\\frac{m_1-m_2}{m_1+m_2},$$\n",
    "\n",
    "as expected. \n",
    "\n",
    "## 7. Markov chains\n",
    "\n",
    "### Chain of random variables\n",
    "\n",
    "What is a Markov Chain? \n",
    "What is a stochastic process\n",
    "\n",
    "\n",
    "## 8. Monte carlo\n",
    "### 8.1 Simulation\n",
    "\n",
    "So far, we have been talking about Monte Carlo simulation. As we recall, the point begind the MC method is to estimate statistics of interest via simulation, which would otherwise be very hard or impossible to calculate analytically. For example, we are familiar that in Bayesian statistics we are many times interested in sampling from a posterior distribution, which is defined by definition as: \n",
    "\n",
    "$$p(B|A)=\\frac{p(A|B)p(B)}{p(A)}=\\frac{p(A|B)p(B)}{\\int p(A|B)p(B)\\,dB}=\\frac{p(A|B)p(B)}{\\int p(A\\cap B)\\,dB}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(8.1)$$\n",
    "\n",
    "Now, many times in Bayesian inference the reason why we need to apply simulation methods is because of the denominator in the above equation, that is $P(A)$. This value is a constant (since $A$ is a given constant value or set of values) but in order to explicitly solve $p(B|A)$ we need to calculate this constant. So what gives? Why don't we just calculate it? Well, it turns out that in many problems this is either very hard or computationally expensive. Just to make things concrete, lets consider a toy example. \n",
    "\n",
    "Assume that we have observed a set of i.i.d. (independent and identically distributed) sample vectors $\\mathcal{Y}=\\{\\textbf{y}_1, \\textbf{y}_2, ..., \\textbf{y}_N\\}, \\textbf{y}_i\\in\\mathcal{R}^K$ from some random vector $\\textbf{y}\\sim \\mathcal{N(\\boldsymbol\\mu, \\boldsymbol\\Sigma})$ where we assume $\\boldsymbol\\mu$ to be unknown variable and $\\boldsymbol\\Sigma$ is a known constant (for simplicity's sake). We wish to calculate some statistic of interest from the posterior distribution of $\\boldsymbol\\mu$. Let that statistic be e.g.: \n",
    "\n",
    "$$E_{\\boldsymbol\\mu|\\mathcal{Y}}\\left[h(\\boldsymbol\\mu)\\right]=E_{\\boldsymbol\\mu|\\mathcal{Y}}\\left[\\sum_{i=1}^K \\mu_i\\right]=\\int \\sum_{i=1}^K \\mu_i \\,p(\\boldsymbol\\mu|\\mathcal{Y})\\,d\\boldsymbol\\mu,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(8.2)$$\n",
    "\n",
    "where $K$ is the dimensionality of $\\boldsymbol\\mu$. So what is the posterior distribution for $\\boldsymbol\\mu$? It is by definition: \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol\\mu|\\mathcal{Y}) = \\frac{p(\\mathcal{Y}|\\boldsymbol\\mu)p(\\boldsymbol\\mu)}{p(\\mathcal{Y})} = \\frac{p(\\mathcal{Y}|\\boldsymbol\\mu)p(\\boldsymbol\\mu)}{\\int p(\\mathcal{Y}|\\boldsymbol\\mu)p(\\boldsymbol\\mu)\\,d\\boldsymbol\\mu},\n",
    "\\end{equation}\n",
    "\n",
    "where I have omitted the $\\boldsymbol\\Sigma$ from the equation since it is a constant (but it is there under the hood). We need next an expression for the prior $p(\\boldsymbol\\mu)$. Assume that we have a Diriclet (multivariable Beta) prior distribution for parameters $\\boldsymbol\\mu$, that is; \n",
    "\n",
    "$$p(\\boldsymbol\\mu)=p(\\mu_1, ..., \\mu_K, \\alpha_1, ..., \\alpha_K) = \\frac{\\Gamma\\left(\\sum_{i=1}^K \\alpha_i\\right)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\\prod_{i=1}^K \\mu_i^{\\alpha_i-1},$$\n",
    "\n",
    "where $\\Gamma$ is the Gamma function and $\\boldsymbol\\alpha=(\\alpha_1, ..., \\alpha_K)$ is some fixed hyperparameter vector for the prior. We have the prior, now we need to solve the likelihood of the observed sample data which is: \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathcal{Y}|\\boldsymbol\\mu) = \\prod_{i=1}^N p(\\textbf{y}_i|\\boldsymbol\\mu) = (2\\pi)^{-\\frac{NK}{2}}\\left|\\boldsymbol\\Sigma\\right|^{-\\frac{N}{2}}\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right),\n",
    "\\end{equation}\n",
    "\n",
    "and it thus follows straightforwardly that the posterior is: \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol\\mu|\\mathcal{Y}) = \\frac{(2\\pi)^{-\\frac{NK}{2}}\\left|\\boldsymbol\\Sigma\\right|^{-\\frac{N}{2}}\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\frac{\\Gamma\\left(\\sum_{i=1}^K \\alpha_i\\right)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}}{\\int (2\\pi)^{-\\frac{NK}{2}}\\left|\\boldsymbol\\Sigma\\right|^{-\\frac{N}{2}}\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\frac{\\Gamma\\left(\\sum_{i=1}^K \\alpha_i\\right)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}\\,d\\boldsymbol\\mu}.\n",
    "\\end{equation}\n",
    "\n",
    "Now, take a look at the denominator distribution: \n",
    "\n",
    "$$p(\\mathcal{Y})=\\int (2\\pi)^{-\\frac{NK}{2}}\\left|\\boldsymbol\\Sigma\\right|^{-\\frac{N}{2}}\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\frac{\\Gamma\\left(\\sum_{i=1}^K \\alpha_i\\right)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}\\,d\\boldsymbol\\mu$$\n",
    "\n",
    "$$= \\int_{\\mu_1}\\int_{\\mu_2}\\cdots\\int_{\\mu_K} (2\\pi)^{-\\frac{NK}{2}}\\left|\\boldsymbol\\Sigma\\right|^{-\\frac{N}{2}}\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\frac{\\Gamma\\left(\\sum_{i=1}^K \\alpha_i\\right)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}\\,d\\mu_1\\,d\\mu_2\\cdots d\\mu_K,$$\n",
    "\n",
    "and assume further that e.g. $K=1000$, which is easily the case with neural networks. This is a monster integral! Or at least to me, this intergal seems a pretty hairy and time consuming task to do, but the point is that **we must calculate it if we insist an explicit formula for the posterior distribution function**, though I bet there probably exists some off-the-shelf formulas to solve this integral. It thus seems that we are badly stuck now, in order to calculate the statistic of equation $(8.2)$ we must solve the posterior distribution first. At this point, we begin to see some hint on where the motivation comes from expressing the posterior distribution as: \n",
    "\n",
    "$$p(B|A)\\propto p(A|B)P(B),$$\n",
    "\n",
    "in Bayesian texts  often. That is, we would like to forget about the normalizing constant. \n",
    "\n",
    "As we have seen, it can be very difficult to calculate statistics of interest from the posterior distribution. In practise, statistics containing integrals are estimated many times by sampling a large number of samples from the posterior distribution and then using these to calculate the value of interest.For example the value in equation $(8.2)$ can be estimated by:  \n",
    "\n",
    "$$E_{\\boldsymbol\\mu|\\mathcal{Y}}\\left[h(\\boldsymbol\\mu)\\right]=\\int \\sum_{i=1}^K \\mu_i \\,p(\\boldsymbol\\mu|\\mathcal{Y})\\,d\\boldsymbol\\mu\\approx \\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^K \\mu_{ij}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(8.3),$$\n",
    "\n",
    "where $\\mu_{ij}$ is the $j$th component of the $i$th sample of a random vector $\\boldsymbol\\mu$. Great! We got rid of the density function! Are we saved? No, this is because of the samples $\\boldsymbol\\mu_i$, we still need the posterior distribution function in order to construct the cumulative distribution function (CDF) which we use for data sampling. What we need now, is a way to sample data points from the posterior distribution without explicitly needing to calculate the posterior distribution and its CDF. \n",
    "\n",
    "Fortunately, there are ways which allow us to sample from the prosterior distribution even though we do not know the normalizing constant denominator. With these approaches, we are able to sample from posterior distriobutions without knowing the CDF. I will next introduce one of the methods that helps us achieve this. \n",
    "\n",
    "\n",
    "### 8.2 Importance sampling\n",
    "\n",
    "Importance sampling is a method which provides a way to sample from nasty looking posterior distributions. To introduce the method, notice that: \n",
    "\n",
    "$$E_f[h(X)]=\\int h(x)f(x)\\,dx = \\int h(x)\\frac{f(x)}{g(x)}g(x)\\,dx = E_g\\left[h(X)\\frac{f(X)}{g(X)}\\right],$$\n",
    "\n",
    "that is we can calculate the statistic of interest (w.r.t. $f$) using some other, perhaps simpler, distribution $g$ from which we can sample easily. In importance sampling, we would simply generate $N$ samples from distribution $g$, and then calculate the values $h(x)\\frac{f(x}{g(x)}$ and average these to get the statistic of interest. Of course, this method still includes the normalizing constant of $f$ we desprately wished to get rid of. Lets make a small alteration to the above equations and we see that: \n",
    "\n",
    "$$E_f[h(X)] = \\frac{ \\int h(x)\\frac{f(x)}{g(x)}g(x)\\,dx }{ \\int \\frac{f(x)}{g(x)}g(x)\\,dx }=\\frac{ \\int h(x)\\frac{f(x)}{g(x)}g(x)\\,dx }{ \\int f(x)\\,dx }= \\int h(x)\\frac{f(x)}{g(x)}g(x)\\,dx.$$\n",
    "\n",
    "In other words, we have that:\n",
    "\n",
    "$$E_f[h(X)] \\approx \\frac{\\frac1n \\sum_{i=1}^n h(x_i)\\frac{f(x_i)}{g(x_i)}}{\\frac1n \\sum_{i=1}^n \\frac{f(x_i)}{g(x_i)}}\\underset{n\\to\\infty}{\\rightarrow}\\frac{ \\int h(x)\\frac{f(x)}{g(x)}g(x)\\,dx }{ \\int \\frac{f(x)}{g(x)}g(x)\\,dx }.$$\n",
    "\n",
    "To give an example of this, lets apply the importance sampling to the nasty example in previous section. Before continuing, lets define few constant values in order to ease the notation and save space. Define: \n",
    "\n",
    "$$\n",
    "A = \\int d\\boldsymbol\\mu = \\int_{\\mu_1}\\int_{\\mu_2}\\cdots\\,d\\mu_1\\,d\\mu_2\\cdots d\\mu_K,\\\\ \n",
    "B = (2\\pi)^{-\\frac{NK}{2}}\\left|\\boldsymbol\\Sigma\\right|^{-\\frac{N}{2}}\\frac{\\Gamma\\left(\\sum_{i=1}^K \\alpha_i\\right)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)},\\\\\n",
    "C = p(\\mathcal{Y}) = \\int_{\\mu_1}\\int_{\\mu_2}\\cdots\\int_{\\mu_K} (2\\pi)^{-\\frac{NK}{2}}\\left|\\boldsymbol\\Sigma\\right|^{-\\frac{N}{2}}\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\frac{\\Gamma\\left(\\sum_{i=1}^K \\alpha_i\\right)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}\\,d\\mu_1\\,d\\mu_2\\cdots d\\mu_K.\n",
    "$$\n",
    "\n",
    "Notice that from these three, only $C$ is uknown. Now we can write the example posterior distribution for $p(\\boldsymbol|\\mathcal{Y})$ as: \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol\\mu|\\mathcal{Y}) = \\frac{B}{C}\\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}.\n",
    "\\end{equation}\n",
    "\n",
    "Let us assume that the new simpler $g$ distribution is a multivariate uniform distribution with some limits. The probability density function is: \n",
    "\n",
    "$$g(\\boldsymbol\\mu) = \\frac{1}{A},$$\n",
    "\n",
    "where $A$ is what we just defined above, that is the multidimensional volume of the distribution. The statistic $h(\\boldsymbol\\mu)$ we wish to calculate is still the same as above. Now we plug the components into the equations of importance sampling and we get: \n",
    "\n",
    "$$E_{\\boldsymbol\\mu|\\mathcal{Y}}\\left[\\sum_{j=1}^K \\mu_i\\right] = E_{\\boldsymbol\\mu}\\left[h(\\boldsymbol\\mu)\\frac{p(\\boldsymbol\\mu|\\mathcal{Y})}{g(\\boldsymbol\\mu)}\\right] =  \\frac{ \\int h(\\boldsymbol\\mu)\\frac{p(\\boldsymbol\\mu|\\mathcal{Y})}{g(\\boldsymbol\\mu)}g(\\boldsymbol\\mu)\\,d\\boldsymbol\\mu }{ \\int \\frac{p(\\boldsymbol\\mu|\\mathcal{Y})}{g(\\boldsymbol\\mu)}g(\\boldsymbol\\mu)\\,d\\boldsymbol\\mu }\\\\[1cm]\n",
    "= \\frac{ \\int \\left(\\sum_{j=1}^K \\mu_j\\right)\\frac{\\frac{B}{C}\\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}}{A^{-1}}A^{-1}\\,d\\boldsymbol\\mu }{ \\int \\frac{\\frac{B}{C}\\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}}{A^{-1}}A^{-1}\\,d\\boldsymbol\\mu }\\\\[1cm]\n",
    "= \\frac{ \\frac{B}{C}\\int \\left(\\sum_{j=1}^K \\mu_j\\right)\\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}\\,d\\boldsymbol\\mu }{\\frac{B}{C} \\int \\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}\\,d\\boldsymbol\\mu }\\\\[1cm]\n",
    "= \\frac{ \\int \\left(\\sum_{j=1}^K \\mu_j\\right)\\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}\\,d\\boldsymbol\\mu }{ \\int \\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu)\\right)\\prod_{i=1}^K \\mu_i^{\\alpha_i-1}\\,d\\boldsymbol\\mu }\\\\[1.5cm]\n",
    "\\approx \\frac{ \\sum_{m=1}^M \\left(\\sum_{i=1}^K \\mu_{mi}\\right)\\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu_m)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu_m)\\right)\\prod_{i=1}^K \\mu_{mi}^{\\alpha_i-1} }{ \\sum_{m=1}^M \\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu_m)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu_m)\\right)\\prod_{i=1}^K \\mu_{mi}^{\\alpha_i-1}},\n",
    "$$\n",
    "\n",
    "where $M$ is the number of realizations from distribution $g(\\boldsymbol\\mu)$ which we can sample very easily (uniform sampling). Voila! We are done! We now have way of simulating values and generating an estimate for the statistic. The above final result looks hairy, I know, but it is computable! We do not have any ugly integrals we need to solve. We simple generate random vectors from $K$-dimensional uniform distribution (assuming we have also $\\mathcal{Y}, \\boldsymbol\\Sigma$ and $\\boldsymbol\\alpha$), plug them into the above formula and we get the estimate. To conclude, we have found that: \n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "E_{\\boldsymbol\\mu|\\mathcal{Y}}\\left[\\sum_{j=1}^K \\mu_i\\right] \\approx \\frac{ \\sum_{m=1}^M \\left(\\sum_{i=1}^K \\mu_{mi}\\right)\\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu_m)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu_m)\\right)\\prod_{i=1}^K \\mu_{mi}^{\\alpha_i-1} }{ \\sum_{m=1}^M \\,\\exp\\left(-\\frac12\\sum_{i=1}^N(\\textbf{y}_i-\\boldsymbol\\mu_m)^T\\boldsymbol\\Sigma^{-1}(\\textbf{y}_i-\\boldsymbol\\mu_m)\\right)\\prod_{i=1}^K \\mu_{mi}^{\\alpha_i-1}}.\n",
    "}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 9. Markov chain Monte Carlo (MCMC)\n",
    "\n",
    "### 9.1 Sampling, convergence, ergodicity\n",
    "\n",
    "## 10. Short intro on Bayesian modeling\n",
    "\n",
    "### 10.1 Bayes formula\n",
    "### 10.2 Bayesian models, predictive distribution\n",
    "### 10.3 Expectation-maximization?...\n",
    "\n",
    "## 11. Hamiltonian Monte Carlo (HMC)\n",
    "\n",
    "## 12. Training deep CNNs via HMC\n",
    "\n",
    "### 12.1 Back propagantion with CNN \n",
    "### 12.2 Constructing the Hamiltonian with CNN\n",
    "\n",
    "## 13. Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
